{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BBCNewsClassification.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOMeO6b7bgEIfWxjNaGOgNO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shashenderkumar/ML/blob/master/BBCNewsClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooq9XLfpXC0V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5519y2mY00c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_train = pd.read_csv('BBC News Train.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CN1TnHghZV5v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "744562f4-eacc-4f82-fffc-8bef35840b3c"
      },
      "source": [
        "data_train.info()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1490 entries, 0 to 1489\n",
            "Data columns (total 3 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   ArticleId  1490 non-null   int64 \n",
            " 1   Text       1490 non-null   object\n",
            " 2   Category   1490 non-null   object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 35.0+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaJ5IUP2ZZO_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "4ad0ccda-333c-472b-df75-2d01a071d256"
      },
      "source": [
        "data_train.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ArticleId</th>\n",
              "      <th>Text</th>\n",
              "      <th>Category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1833</td>\n",
              "      <td>worldcom ex-boss launches defence lawyers defe...</td>\n",
              "      <td>business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>154</td>\n",
              "      <td>german business confidence slides german busin...</td>\n",
              "      <td>business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1101</td>\n",
              "      <td>bbc poll indicates economic gloom citizens in ...</td>\n",
              "      <td>business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1976</td>\n",
              "      <td>lifestyle  governs mobile choice  faster  bett...</td>\n",
              "      <td>tech</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>917</td>\n",
              "      <td>enron bosses in $168m payout eighteen former e...</td>\n",
              "      <td>business</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   ArticleId                                               Text  Category\n",
              "0       1833  worldcom ex-boss launches defence lawyers defe...  business\n",
              "1        154  german business confidence slides german busin...  business\n",
              "2       1101  bbc poll indicates economic gloom citizens in ...  business\n",
              "3       1976  lifestyle  governs mobile choice  faster  bett...      tech\n",
              "4        917  enron bosses in $168m payout eighteen former e...  business"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjlvyOIucP9n",
        "colab_type": "text"
      },
      "source": [
        "## **Split data in train and test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JX_ayXRvcUNx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiKP-Zh8d1Um",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "ec000cd7-6978-4a62-ed6c-1045ad8856c2"
      },
      "source": [
        "X = data_train['Text'].values\n",
        "# X = data_train.iloc[0:2].values\n",
        "y = data_train['Category'].values\n",
        "\n",
        "y_t = np.array(y)\n",
        "# integer encode\n",
        "le = LabelEncoder()\n",
        "integer_encoded = le.fit_transform(y_t)\n",
        "# binary encode\n",
        "ohe = OneHotEncoder(sparse=False)\n",
        "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
        "y_t1 = ohe.fit_transform(integer_encoded)\n",
        "print(y_t1)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " ...\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9szkqA3i2Zm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y_t1, test_size = 0.10, random_state = 1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9hL4x445jYB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "3f989b4c-a591-46c5-9797-1cd184617739"
      },
      "source": [
        "print(X_train.size)\n",
        "print(X_test.size)\n",
        "print(len(data_train))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1341\n",
            "149\n",
            "1490\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhAFm-wQjIqH",
        "colab_type": "text"
      },
      "source": [
        "### **Tokenize Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTFMIBasjMUG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "75026ae5-ca2b-4c7c-b62d-098901c0b6d4"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLk-V27vjakH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer(num_words=5000)\n",
        "\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train1 = tokenizer.texts_to_sequences(X_train)\n",
        "X_test1 = tokenizer.texts_to_sequences(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "335XMHFpj7FV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eO7BSWm4kEpZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9320b0b1-6027-411c-dcef-0efb87f4373c"
      },
      "source": [
        "print(vocab_size)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "23910\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTya2wlgkH7p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "db19f702-d763-4183-aac2-055642ddae99"
      },
      "source": [
        "print(X_train[1:6])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['dollar hovers around record lows the us dollar hovered close to record lows against the euro on friday as concern grows about the size of the us budget deficit.  analysts predict that the dollar will remain weak in 2005 as investors worry about the state of the us economy. the bush administration s apparent unwillingness to intervene to support the dollar has caused further concern. however  trading has been volatile over the past week because of technical and automated trading and light demand. this has amplified reactions to news  analysts said  adding that they expect markets to become less jumpy in january.  the dollar was trading at $1.3652 versus the euro on friday morning after hitting a fresh record low of $1.3667 on thursday. one dollar bought 102.55 yen.  disappointing business figures from chicago triggered the us currency s weakness on thursday. the national association of purchasing management-chicago said its manufacturing index dropped to 61.2  a bigger fall than expected.  there are no dollar buyers now  especially after the chicago data yesterday   said abn amro s paul mackel. at the same time  german chancellor gerhard schroeder and italian prime minister silvio berlusconi voiced concerns about the strength of the euro. mr berlusconi said the euro s strength was  absolutely worrying  for italian exports. mr schroeder said in a newspaper article that stability in foreign exchange markets required a correction of global economic imbalances. investors will now look towards february s meeting of finance ministers from the g7 industrialised nations in london for clues as to whether central banks will combine forces to stem the dollar s decline.'\n",
            " 'england claim dubai sevens glory england beat fiji 26-21 in a dramatic final in dubai to win the first irb sevens event of the season.  having beaten australia and south africa to reach the final  england fell behind to an early try against fiji. they then took charge with scores from pat sanderson  kai horstman  mathew tait and rob thirlby  but fiji rallied to force a tense finale. scotland were beaten 33-15 by samoa in the plate semi-final and ireland lost 17-5 to tunisia in the shield final. mike friday s england side matched their opponents for pace  power and skill in the final and led 19-7 at half-time. but neumi nanuku and marika vakacegu touched down for fiji  only for a needless trip by tuidriva bainivalu on geoff appleford to allow england to run down the clock.  to be honest  england have wanted to win in dubai for a very long time now  and the people here have wanted us to win for just as long   said friday.   we didn t want to put pressure on ourselves but we are thankful we have achieved that and brought through some young talent at the same time that can hopefully play for the england  15s  in a few years.  portugal confirmed their impressive progress in sevens rugby by recording a sudden-death win over france in the bowl final. samoa won the plate title by edging out argentina 21-19.'\n",
            " 'a year to remember for irish there used to be one subliminal moment during a year in irish rugby that stood out more than most.  well  at least there used to one. now there is a handful to look back with a mixture of satisfaction  and sorrow. it has been quite a year for the irish  and not just with eddie o sullivan s triple crown winning international outfit either. right down through the ranks irish rugby is creating waves and upsetting the more established teams in the game. but most of the kudos will go to o sullivan and his merry band of warriors who not only collected their first triple crown for 29 years  but also finished their autumn campaign with a 100% record. for the second year in succession they also finished in the runners-up spot in the rbs six nations. but in the three games in november which included a victory over tri-nations champions and grand slam chasing south africa  ireland finsihed the year on a high. the 18-12 victory at lansdowne road was only their second victory over the boks after the initial success back in 1965. that success was revenge for the consecutive defeats in blomefontein and cape town in the summer. those two reverses and the 35-17 flop against france  were the only dark patches in an otherwise excellent 12 months. but the big one  of course  was the 19-13 defeat of world cup champions england on their precious twickenham turf. the winning try was conceived in o sullivan s mind  perfectly executed by the team and finished immaculately by girvan dempsey. for me  the try of the championship. o sullivan s career is now in vertical take-off mode. it is no wonder that sir clive woodward has elevated the galway-based coach to head the lions test side. not only that  but a fair majority of the present ireland side will be wearing red next june in new zealand.  there can be no doubt that ireland s representation will be the biggest ever  albeit in a proposed 44-man squad. in brian o driscoll and paul o connell  ireland have now the two front-runners for the captaincy. gordon d arcy  whose career began as a teenager back in 1999  finally arrived when he was named the six nations player of the tournament. but it was not only the senior squad that brought kudos to ireland  the youngsters strutted their stuff on the big stage as well. the under-21 squad confounded the doubters as they went all the way to the world cup final in scotland only to be beaten by a powerful all black side in the decider. the young irish boys had stated their intentions earlier in the season when they finished runners-up to england in the six nations under-21 tournament. on the provincial front  leinster  for second year in succession  blew it when the heineken cup looked a good wager. while ulster finished runners-up in their very tight group for the second season in succession  it was munster again flying the flag for the irish. looking to reach their third final  they went down 37-32 to eventual winners wasps in what many beileve was the most competitive and thunderous game ever witnessed at lansdowne road. how wasps recovered from that energy-sapping duel  and then go onto to defeat toulouse in the final was anybody s guess. ulster  meanwhile  just lost out to adding the inaugural celtic cup in winning the celtic league when they were pipped at the post by the scarlets in the final game.  ulster  however  took time to start the new season under new coach mark mccall. the once famous ravenhill fortress was breached four times as ulster only manged five wins from their first 12 outings in the celtic league. leinster are again looking the most potent outfit going into 2005  but whether they can take that final step under declan kidney is another thing. on the down side  irish rugby was hit by a number of tragedies. teenage star john mccall died while playing for the ireland against new zealand in the under-19 world cup game in durban. that happened only 10 days after he led royal armagh to their first ulster schools  cup success since 1977. the death of former ireland coach and lions flanker mike doyle in a car crash in northern ireland shocked the rugby fraternity a larger than life character  doyle had coached ireland to the triple crown in 1985  the last time that goal had been achieved before this season. ulster rugby also suffered the sudden deaths of well-known londonderry ym player jim huey  coleraine s jonathan hutchinson  and belfast harlequins lock johnny poole. they all passed away long before the full-time whistle.'\n",
            " 'khodorkovsky ally denies charges a close associate of former yukos boss mikhail khodorkovsky has told a court that fraud charges levelled against him are  false .  platon lebedev has been on trial alongside mr khodorkovsky since june in a case centring around the privatisation of a fertiliser firm. the pair claim they are being punished by the authorities for the political ambitions of mr khodorkovsky. mr lebedev said there were  absurd contradictions  in the case. opening his defence  he said he could not see the legal basis of the charges he faced  which also include allegations of tax evasion.  to my embarrassment  i could not understand the file of complaints against me   he told a moscow court. mr lebedev headed the menatep group  the parent company of yukos.  mr lebedev and mr khodorkovsky  who each face a possible 10 year jail sentence if convicted  will be questioned by a judge over the next few days. mr khodorkovsky began his testimony last week  telling the court that he objected to the way that the  running of a normal business has been presented as a work of criminal fiction . the charges are seen by supporters as politically motivated and part of a drive by russian president vladimir putin to rein in the country s super-rich business leaders  the so-called oligarchs. yukos has been presented with a $27.5bn (£13bn) tax demand by the russian authorities and its key yugansk division was auctioned off to part settle the bill. the company s effort to gain bankruptcy protection in the us - in a bid to win damages for the sale - were dismissed by a court in texas.'\n",
            " 'england given tough sevens draw england will have to negotiate their way through a tough draw if they are to win the rugby world cup sevens in hong kong next month.  the second seeds have been drawn against samoa  france  italy  georgia and chinese taipei. the top two sides in each pool qualify but england could face 2001 winners new zealand in the quarter-finals if they stumble against samoa. scotland and ireland are in pool a together with the all blacks. england won the first event of the international rugby board world sevens series in dubai but have slipped to fourth in the table after failing to build on that victory.  however  they beat samoa in the recent los angeles sevens before losing to argentina in the semi-finals.  england have the ability and determination to win this world cup and create sporting history by being the only nation to hold both the 15s and sevens world cups at the same time   said england sevens coach mike friday.  england have a fantastic record in hong kong and have won there the last three years  but the world cup is on a different level.  every pool contains teams who have caused upsets before and we will have to work hard to ensure we progress from our group.  we have not performed consistently to our true potential so far in the irb sevens which has been disappointing - but we can only look forward.  england won the first rugby world cup sevens in 1993 with a side that included the likes of lawrence dallaglio and matt dawson. in 1997 and 2001  england lost in the quarter-finals.  (seeds in brackets)  new zealand (1)  scotland (8)  tonga  ireland  korea  usa.  england (2)  samoa (7)  france  italy  georgia  chinese taipei.  fiji (3)  australia (6)  canada  portugal  japan  hong kong.  argentina (4)  south africa (5)  kenya  tunisia  russia  uruguay.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJKLVVzakWkn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "856fe297-4360-4119-9d35-129f5b048c35"
      },
      "source": [
        "print(X_train1[0])\n",
        "print(X_train1[1])\n",
        "print(X_train1[2])\n",
        "print(X_train1[3])\n",
        "print(X_train1[4])\n",
        "print(X_train1[5])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2888, 97, 448, 8, 164, 1713, 164, 19, 293, 1, 670, 21, 5, 449, 161, 991, 1, 1273, 331, 11, 14, 1, 79, 535, 2, 132, 1644, 453, 2888, 97, 3, 1, 43, 4, 89, 554, 97, 21, 1, 331, 1645, 22, 409, 221, 350, 1680, 1, 151, 3615, 1713, 4185, 164, 726, 1231, 2, 1, 143, 3, 1, 73, 161, 1862, 12, 42, 703, 6, 1067, 72, 1084, 823, 749, 3269, 3783, 1863, 37, 266, 1, 117, 690, 8, 638, 855, 79, 489, 66, 148, 11, 14, 2156, 1144, 22, 2045, 123, 4, 179, 273, 123, 37, 25, 186, 2, 16, 214, 1602, 64, 29, 25, 321, 34, 243, 23, 29, 956, 54, 12, 1, 855, 8, 1, 117, 690, 97, 3, 209, 37, 335, 2, 1, 1274, 535, 1, 2479, 3, 1, 844, 489, 6, 1864, 5, 48, 2781, 6, 323, 161, 4, 1481, 3112, 1998, 11, 824, 2, 4720, 1, 4186, 3, 1, 63, 1345, 164, 14, 1740, 61, 185, 83, 4, 9, 428, 2, 20, 398, 248, 3113, 1, 143, 1865, 8, 1, 89, 297, 97, 3, 1, 43, 335, 2, 1, 3, 4452, 27, 3270, 14, 1562, 1, 2577, 97, 3, 133, 42, 703, 14, 4453, 22, 164, 37, 14, 489, 12, 1, 289, 148, 11, 14, 181, 91, 110, 697, 22, 1527, 4, 14, 2992, 8, 42, 37, 27, 1, 87, 489, 248, 1, 289, 67, 1866, 2993, 14, 1085, 143, 3, 1, 43, 351, 1, 2201, 27, 1274, 37, 140, 6, 249, 1, 671, 331, 25, 1563, 8, 22, 40, 60, 1390, 101, 2668, 3, 323, 4, 309, 161, 161, 331, 103, 34, 20, 2994, 6, 3114, 61, 1, 66, 187, 83, 1, 73, 125, 8, 161, 1903, 22, 605, 4, 14, 974, 5, 217, 1346, 6, 249, 220, 2, 5, 337, 258, 22, 368, 868]\n",
            "[395, 248, 217, 1, 49, 395, 565, 2, 217, 80, 1, 810, 12, 505, 18, 1319, 4454, 53, 1, 1413, 3, 1, 49, 415, 811, 368, 2889, 10, 1, 395, 24, 744, 1777, 6, 209, 18, 845, 2669, 53, 1, 356, 3, 1, 49, 218, 1, 856, 2098, 7, 3616, 2, 2, 401, 1, 395, 19, 1564, 380, 1319, 181, 1045, 19, 39, 61, 1, 399, 148, 109, 3, 1414, 4, 1045, 4, 1211, 503, 34, 19, 2, 197, 368, 13, 1157, 10, 29, 1212, 825, 2, 381, 375, 6, 310, 1, 395, 14, 1045, 21, 107, 1, 810, 12, 505, 1826, 52, 3115, 5, 1944, 217, 438, 3, 107, 12, 677, 50, 395, 945, 3617, 3116, 2099, 2046, 251, 298, 27, 2782, 1, 49, 1827, 7, 3977, 12, 677, 1, 301, 931, 3, 1068, 2782, 13, 42, 1603, 1604, 1275, 2, 123, 5, 1453, 769, 60, 186, 51, 25, 82, 395, 2670, 70, 1109, 52, 1, 2782, 324, 13, 7, 718, 21, 1, 289, 67, 834, 433, 3784, 4, 1528, 290, 157, 4721, 932, 53, 1, 1714, 3, 1, 810, 31, 13, 1, 810, 7, 1714, 14, 1646, 3618, 8, 1528, 1145, 31, 3784, 13, 6, 5, 915, 3271, 10, 3272, 6, 299, 869, 825, 2578, 5, 3, 524, 267, 845, 24, 70, 429, 1232, 470, 7, 461, 3, 1026, 639, 27, 1, 3273, 4455, 280, 6, 193, 8, 18, 2, 386, 902, 1251, 24, 4456, 2783, 2, 1, 395, 7, 1778]\n",
            "[122, 691, 3978, 3432, 2995, 122, 566, 1391, 1252, 6, 5, 3433, 237, 6, 3978, 2, 132, 1, 63, 3432, 590, 3, 1, 366, 430, 1945, 791, 4, 357, 640, 2, 916, 1, 237, 122, 720, 471, 2, 33, 315, 387, 80, 29, 134, 266, 933, 17, 3619, 27, 3274, 4, 3434, 23, 2, 975, 5, 4722, 382, 44, 1945, 3275, 581, 22, 6, 1, 2270, 237, 4, 264, 328, 665, 205, 2, 6, 1, 237, 870, 505, 7, 122, 277, 3785, 38, 3435, 8, 1605, 410, 4, 6, 1, 237, 4, 625, 846, 255, 21, 179, 67, 23, 4, 124, 8, 79, 8, 5, 1715, 22, 12, 3979, 2, 555, 122, 2, 378, 124, 1, 4723, 2, 16, 3980, 122, 20, 690, 2, 132, 6, 3978, 8, 5, 91, 219, 67, 70, 4, 1, 47, 504, 20, 690, 49, 2, 132, 8, 81, 18, 219, 13, 505, 32, 735, 68, 159, 2, 196, 666, 12, 3436, 23, 32, 25, 32, 20, 2202, 10, 4, 1069, 166, 74, 549, 1779, 21, 1, 289, 67, 10, 55, 1999, 156, 8, 1, 122, 6, 5, 346, 83, 976, 38, 1565, 1529, 6, 3432, 582, 22, 2271, 5, 1046, 132, 61, 253, 6, 1, 3276, 237, 140, 1, 535, 22, 54, 1946, 1252, 846]\n",
            "[5, 43, 2, 2335, 8, 750, 51, 175, 2, 16, 50, 917, 198, 5, 43, 6, 750, 582, 10, 2480, 54, 40, 60, 117, 110, 21, 545, 51, 175, 2, 50, 70, 51, 9, 5, 2, 429, 98, 17, 5, 3, 4, 11, 19, 39, 934, 5, 43, 8, 1, 750, 4, 28, 81, 17, 3437, 329, 1298, 7, 3438, 2201, 453, 226, 727, 215, 124, 166, 1, 4724, 750, 582, 9, 2000, 4457, 4, 1, 40, 2784, 1947, 6, 1, 97, 23, 117, 3, 1, 24, 153, 2, 329, 1298, 4, 30, 393, 3, 46, 28, 79, 3620, 38, 63, 3438, 2201, 8, 1867, 83, 23, 45, 1681, 38, 2398, 294, 17, 5, 605, 217, 8, 1, 144, 43, 6, 2785, 29, 45, 1681, 6, 1, 2786, 41, 1392, 6, 1, 3786, 187, 280, 23, 6, 1, 92, 161, 6, 518, 37, 823, 5, 558, 61, 280, 798, 4, 749, 1171, 3787, 357, 640, 264, 1, 43, 12, 5, 167, 1, 562, 330, 558, 21, 3277, 1189, 14, 79, 38, 144, 558, 61, 1, 52, 1, 2399, 574, 98, 6, 10, 574, 14, 8, 1, 3117, 6, 4, 3621, 6, 1, 736, 150, 65, 4, 1, 1948, 665, 80, 253, 44, 1, 79, 3622, 4187, 6, 33, 3278, 2481, 330, 200, 23, 1, 214, 50, 3, 885, 14, 1, 846, 799, 1027, 3, 69, 287, 798, 122, 12, 38, 2671, 1, 453, 387, 14, 6, 329, 1298, 7, 1253, 3279, 22, 1, 224, 4, 1681, 22, 4725, 8, 195, 1, 387, 3, 1, 1454, 329, 1298, 7, 575, 9, 70, 6, 115, 136, 11, 9, 82, 3118, 10, 728, 2100, 1904, 19, 1, 240, 446, 2, 606, 1, 1431, 764, 277, 28, 79, 10, 23, 5, 1320, 886, 3, 1, 1321, 264, 277, 24, 16, 3981, 1299, 105, 835, 6, 48, 857, 51, 55, 16, 82, 1158, 10, 264, 7, 24, 16, 1, 325, 472, 6, 5, 1566, 2579, 332, 935, 6, 1530, 329, 2336, 4, 718, 329, 3623, 264, 20, 70, 1, 65, 887, 2786, 8, 1, 755, 567, 946, 575, 1129, 18, 5, 3982, 98, 6, 1507, 1741, 2400, 64, 15, 14, 1085, 1, 187, 280, 369, 3, 1, 1322, 23, 11, 14, 28, 79, 1, 781, 935, 10, 1069, 2, 264, 1, 4188, 38, 2337, 12, 1, 214, 737, 18, 110, 1, 168, 1252, 935, 1, 18, 29, 335, 57, 1, 118, 2, 1, 69, 287, 237, 6, 382, 79, 2, 16, 1945, 22, 5, 1393, 57, 977, 277, 6, 1, 4458, 1, 549, 750, 2157, 36, 4459, 38, 379, 6, 1, 366, 64, 29, 1681, 2786, 41, 2, 122, 6, 1, 187, 280, 168, 1252, 1322, 12, 1, 887, 3439, 8, 144, 43, 6, 2785, 11, 64, 1, 2580, 287, 871, 5, 128, 106, 2272, 1681, 2786, 41, 6, 38, 91, 2787, 180, 8, 1, 144, 366, 6, 2785, 11, 14, 2996, 358, 2338, 1, 8, 1, 750, 352, 2, 916, 38, 232, 237, 29, 335, 124, 2890, 2101, 2, 1084, 2672, 6, 78, 100, 14, 1, 117, 1508, 4, 97, 472, 21, 3277, 1189, 139, 2672, 4189, 27, 10, 1159, 4, 134, 153, 1567, 2, 1027, 6, 1, 237, 14, 3788, 7, 2272, 888, 81, 328, 54, 2, 1157, 1, 1742, 287, 6, 453, 1, 1742, 513, 64, 29, 44, 21, 1, 1086, 22, 1, 6, 1, 237, 97, 2272, 181, 266, 67, 2, 268, 1, 48, 366, 168, 48, 446, 519, 1, 442, 2158, 14, 185, 353, 18, 2272, 79, 172, 1047, 27, 38, 63, 330, 6, 1, 1742, 513, 3439, 25, 358, 352, 1, 117, 141, 71, 209, 23, 386, 29, 55, 115, 10, 237, 889, 168, 9, 259, 630, 12, 1, 124, 277, 750, 582, 14, 256, 22, 5, 104, 3, 3119, 350, 390, 1233, 106, 388, 8, 1, 264, 80, 48, 857, 6, 1, 168, 846, 69, 287, 97, 6, 10, 1254, 79, 169, 313, 52, 15, 625, 1568, 2, 38, 63, 2272, 836, 287, 574, 146, 1, 1046, 3, 206, 264, 446, 4, 1431, 2673, 870, 6, 5, 756, 2997, 6, 1682, 264, 3624, 1, 582, 5, 1868, 60, 273, 2047, 36, 264, 2, 1, 3438, 2201, 6, 2998, 1, 66, 67, 10, 583, 36, 39, 2202, 95, 34, 366, 2272, 582, 45, 1415, 1, 3, 110, 506, 369, 2482, 7, 2159, 4, 4726, 2674, 2483, 29, 57, 1531, 293, 219, 95, 1, 394, 67]\n",
            "[2160, 3120, 826, 5, 565, 3, 206, 602, 704, 3625, 2160, 19, 90, 5, 288, 10, 681, 826, 80, 126, 25, 2273, 19, 39, 12, 713, 1828, 31, 2160, 146, 835, 6, 5, 402, 248, 1, 3, 5, 137, 1, 1070, 691, 29, 25, 88, 22, 1, 918, 8, 1, 501, 2401, 3, 31, 2160, 31, 13, 51, 44, 6, 1, 402, 751, 30, 992, 15, 13, 15, 58, 28, 165, 1, 364, 1569, 3, 1, 826, 15, 1829, 37, 45, 507, 1606, 3, 227, 2, 130, 26, 58, 28, 1394, 1, 993, 3, 2274, 80, 195, 15, 90, 5, 288, 31, 2788, 1, 180, 1, 2789, 135, 3, 602, 31, 4, 31, 2160, 46, 424, 391, 5, 647, 169, 43, 1949, 2891, 59, 2484, 24, 16, 1905, 22, 5, 1532, 61, 1, 105, 346, 313, 31, 2160, 1129, 30, 66, 148, 2581, 1, 288, 10, 15, 2, 1, 118, 10, 1, 576, 3, 5, 2161, 251, 19, 39, 1645, 18, 5, 152, 3, 1087, 2892, 1, 826, 25, 311, 22, 1130, 18, 2999, 4, 160, 3, 5, 714, 22, 705, 326, 3789, 2, 6, 1, 177, 7, 1482, 2485, 251, 1131, 1, 77, 257, 602, 19, 39, 1645, 17, 5, 1172, 1190, 227, 503, 22, 1, 705, 918, 4, 42, 454, 2275, 1780, 14, 136, 2, 160, 2675, 1, 347, 1, 135, 7, 1048, 2, 1716, 1071, 1132, 6, 1, 49, 6, 5, 584, 2, 132, 2001, 8, 1, 411, 44, 2339, 22, 5, 288, 6, 2340]\n",
            "[122, 338, 890, 3432, 1255, 122, 24, 20, 2, 4460, 38, 118, 166, 5, 890, 1255, 59, 29, 25, 2, 132, 1, 582, 69, 287, 3432, 6, 2676, 2402, 105, 243, 1, 144, 20, 39, 2048, 80, 253, 620, 3790, 4, 1072, 1, 143, 65, 1416, 6, 424, 23, 122, 58, 391, 591, 1084, 48, 857, 6, 1, 443, 1717, 59, 29, 80, 382, 4, 264, 25, 6, 5, 678, 17, 1, 57, 2341, 122, 140, 1, 63, 590, 3, 1, 226, 582, 715, 69, 3432, 419, 6, 3978, 23, 20, 3000, 2, 621, 6, 1, 2049, 52, 1906, 2, 1049, 12, 10, 558, 181, 29, 566, 6, 1, 337, 1533, 1781, 3432, 95, 1160, 2, 1946, 6, 1, 2270, 1717, 122, 20, 1, 1110, 4, 3440, 2, 132, 34, 69, 287, 4, 738, 2677, 770, 22, 88, 1, 79, 1647, 2, 556, 170, 1, 4, 3432, 69, 21, 1, 289, 67, 13, 122, 3432, 446, 870, 505, 122, 20, 5, 1782, 217, 6, 2676, 2402, 4, 20, 140, 51, 1, 66, 92, 83, 23, 1, 69, 287, 9, 12, 5, 474, 525, 343, 3121, 1947, 46, 20, 1564, 95, 4, 32, 24, 20, 2, 152, 339, 2, 837, 32, 1529, 27, 127, 180, 32, 20, 28, 1907, 4461, 2, 127, 1455, 936, 77, 322, 6, 1, 3432, 37, 19, 39, 2046, 23, 32, 55, 79, 429, 550, 122, 140, 1, 63, 582, 69, 287, 3432, 6, 2893, 17, 5, 277, 10, 823, 1, 2582, 3, 3626, 1648, 4, 1680, 2894, 6, 1649, 4, 591, 122, 328, 6, 1, 443, 1717, 6, 48, 857, 107, 382, 462, 264, 1570, 2583, 122, 123, 255, 253, 620, 3790, 1072, 164, 791, 145, 2203, 603, 2676, 2402, 1946, 207, 357, 640, 205, 903]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2OF_lCFk654",
        "colab_type": "text"
      },
      "source": [
        "**PADS Sequence**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZ_SpbEjk-7w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Omh5uzCklIPT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# have to check the value of this hyperparameter\n",
        "maxlen = 1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqsK5jS4ks4f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train1 = pad_sequences(X_train1, padding= 'post', maxlen = maxlen)\n",
        "X_test1 = pad_sequences(X_test1, padding= 'post', maxlen = maxlen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzVG3MFQno8X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "05e3a7d7-84d8-4f32-9f70-7a43368c8088"
      },
      "source": [
        "print(X_train1[0])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2888   97  448    8  164 1713  164   19  293    1  670   21    5  449\n",
            "  161  991    1 1273  331   11   14    1   79  535    2  132 1644  453\n",
            " 2888   97    3    1   43    4   89  554   97   21    1  331 1645   22\n",
            "  409  221  350 1680    1  151 3615 1713 4185  164  726 1231    2    1\n",
            "  143    3    1   73  161 1862   12   42  703    6 1067   72 1084  823\n",
            "  749 3269 3783 1863   37  266    1  117  690    8  638  855   79  489\n",
            "   66  148   11   14 2156 1144   22 2045  123    4  179  273  123   37\n",
            "   25  186    2   16  214 1602   64   29   25  321   34  243   23   29\n",
            "  956   54   12    1  855    8    1  117  690   97    3  209   37  335\n",
            "    2    1 1274  535    1 2479    3    1  844  489    6 1864    5   48\n",
            " 2781    6  323  161    4 1481 3112 1998   11  824    2 4720    1 4186\n",
            "    3    1   63 1345  164   14 1740   61  185   83    4    9  428    2\n",
            "   20  398  248 3113    1  143 1865    8    1   89  297   97    3    1\n",
            "   43  335    2    1    3 4452   27 3270   14 1562    1 2577   97    3\n",
            "  133   42  703   14 4453   22  164   37   14  489   12    1  289  148\n",
            "   11   14  181   91  110  697   22 1527    4   14 2992    8   42   37\n",
            "   27    1   87  489  248    1  289   67 1866 2993   14 1085  143    3\n",
            "    1   43  351    1 2201   27 1274   37  140    6  249    1  671  331\n",
            "   25 1563    8   22   40   60 1390  101 2668    3  323    4  309  161\n",
            "  161  331  103   34   20 2994    6 3114   61    1   66  187   83    1\n",
            "   73  125    8  161 1903   22  605    4   14  974    5  217 1346    6\n",
            "  249  220    2    5  337  258   22  368  868    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7LyeikPnu_F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a3714528-44df-4b3f-d43f-88fbcb7658ec"
      },
      "source": [
        "print(X_train1[1057])\n",
        "print(X_train1[1058])\n",
        "print(X_train1[1059])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1408    2 1699   54   12    1  191   22  652    3    1   73    7 1665\n",
            "   24  131   16  300  283  510   21  129  120    5 1420  248 1276  155\n",
            " 2128   24 1699   54   12    5  938  729    3 2359  222   94   18  880\n",
            "    4 2129  276 3024    1  258   22 1677  739  852   11    2 1172  155\n",
            "   56  557    3    1   73   46   25   28  412  297    1  772   10    1\n",
            "  233 4368   24   17   67    9 1911    1  258    1 1420  628   12    1\n",
            "   76    4 1677  263    2  223   41   17   48 1091    2 4823  150   10\n",
            "   20   39   22    1  233 2752  312    1 2932    3 2128  300  129  510\n",
            "   24   20 1776 2002  150   10  744  233   24 1699   54   12   40    1\n",
            "  258 1147   18   40    4   40 4928  279  297    4  222  381  375    1\n",
            " 4368   24  381   40 2148    4   40    8  150   10   20   28  265  191\n",
            "  510   11 3024    1 2250  162   20 2621    4   20   19   39  151 4629\n",
            "   53   23 3403   53  139   94    5 4368   24 2834  296   19   39  375\n",
            " 3325  852  108   54    2 2889  296 4619  240   12  392  276    4  351\n",
            " 1103    3    1  118  173    9 2144   46 2889   10    4    1    3   40\n",
            " 1823 2435  173   24 2357    1  233 4368   58   16  118  136  519    1\n",
            "  258 1147  283  510   12   72  722    2   16  468  456   41   22  150\n",
            "   46  190   20   11   13  216    3 1382 1581   21  852  520    3  283\n",
            "  510   20 1776    4 3954    6 3025  873   20 2068   61    1   66   43\n",
            "   23    1  436  922   51    9    5  339    3   20   46   25   28   17\n",
            "    1  191   29  238   68   20    1   56 2150   56    1 1819   13   31\n",
            "   18   70    1  117  864   25  319    2  744  457  438 1014 1400    1\n",
            " 2003  984    4    1 3993  150   12  438 3288   24 1103    8    5  443\n",
            "    3    1  233   20    1 3993   24   85   41  787    4    1 3287 1118\n",
            "    5  232   22    1  258 1838 2138   94   18  852   20    5 1581    2\n",
            "  210 1306    1  508   13   31    1   19  311 1119  780   17   42 2283\n",
            "  297  754   37 1444  283  510    2   47    6  641 2192  248  221    6\n",
            "   50  962    3   17  167  966    3 1519  297 1080  824   47    4 2564\n",
            " 2150    2   96   48  527 2004   31   94 1730    1 2358  804    3 1266\n",
            " 2192    9 2359    9    1  508    3    1  233 4368    9    2   16 4031\n",
            "   15   13   59   32  238   68 1419   34  508   70   11   24   96    5\n",
            "  340 1901    4   47   24  450   11   40  636    2  450  527  880 1522\n",
            "   24   16 1333    4   29  832  919   28   16  307    2  499   41   17\n",
            " 1349   15   13    1 1979    8  233   33  760  947   17  569 2048   27\n",
            "   76  263    4    1  860   19  661   39  108   41    2 1306   74    3\n",
            "    1  559 1829   22    1  233    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0]\n",
            "[1777  395  615    1   69    7  325  137   19  694    5 3648  354    6\n",
            "  615   52    1 1777  395  266    5   54    3   42  780  748  227  615\n",
            "  730  107    2  107    6  133   23   35   20   39  462  707   59 1827\n",
            " 4885   44 4002   54    1 2021    3 2682   94   18 1083    4   40   60\n",
            " 1517    3   42  202  899    1   73   13   11   14 1178   11   35  256\n",
            "   42 1696    8  209  106    1 3934 1267 1686  922 1508   32   25 1178\n",
            "   10   32   20    1 1653 2682    4   47    2 1550  552  127  583    6\n",
            "  209   13  225  361    1 3648  959  354   36   39  186   22  368   52\n",
            "    1  135   13    6  373   10    1  981  736 2694   36  256 3670 2695\n",
            "  202    6  242   13   42  202   44   41   22  207    6  133  204   14\n",
            "  824   22   42 2682  234    4   37 2683    5 1192   43   17  103    8\n",
            "  103  202   41  145 3914  202   44   41  123   17  365  204    6   49\n",
            " 3670 3914  625   22 1083    4 3914   22    1 4970  202    6  242  119\n",
            "   10   42 1480    8  204  398 1802  526   36 2612    6  133 1495  593\n",
            "  398 2205    2  146    1 1226 1129    6 2113  249    1  526    9  108\n",
            "    2  565  342    3    1  180    7    4  169    3    1 4590 4408   40\n",
            "   60  557  101   47 1485   17   53  255  101    6    1   73    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0]\n",
            "[2466    8  272 2142  271    5  171  637  702   19 2466    5    3    1\n",
            "  914    8 1244   48  171  586  775   37  323  240 3957    1  364 1361\n",
            "  702   13    1  531  102  252 4293    1  323 4128 3957 2264   52  614\n",
            "    2   98   11   11   19   36 4745  969   46  154   11   58 1446  613\n",
            "   61  485  246    4 1034  245 1505  235 2387 1130  154   11   35  607\n",
            "  246 1371   38 3957    1 2264    9 1974    2  304 2142 1132    2 3957\n",
            "   10  142  235    2 2822   38 1359    6   72 1807  323 4128    1 2467\n",
            "  271 1415   64   50    3    1  758  272  774 1039 1374   42 4124 1644\n",
            "    6   65  200   12    1  477   19  709    2 1716 3798    6   74  301\n",
            " 1518   84  168  666   79   65 1463    1 2467  271   21    1  461   17\n",
            "   50 1891    2 3435    3    1 2467 2264 2491    1  284    4   13    5\n",
            "   48   63 1955    3    1  914   35  314    1  272    5  406    2   20\n",
            " 3146   53   42 4861    6   57  774 1039    6    1   49    1    3  323\n",
            "  972    4  283  251    9   34  530   10    1   49  240 2821 1638 2883\n",
            "    5 2142    8   42   50 3101 2861  194    8 1170  969   25  949   10\n",
            "    1 2264   58  389    2    5  717 1574 3221    6  242   34   29 1297\n",
            "   58 2610  485  235 1841  109   29   86   28   20    1  364    4  355\n",
            "  487    3 1868  239   59   29   36    2  970 2142  364  281    6  288\n",
            " 1130  154  392 1052   25    4   11   35 1269    2  176   41    5  388\n",
            " 1036  300 1495  272 1052    6  278   17    1   49    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nosDIRCFoWtQ",
        "colab_type": "text"
      },
      "source": [
        "### **Model Creation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nM1p9sqWoZwf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "m = tf.keras.models\n",
        "layers = tf.keras.layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6ALYEXVowuN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = m.Sequential()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFnu3UzOo6KR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "a5787356-79ed-49c9-de89-cc15b822a786"
      },
      "source": [
        "# vocab size is 23910\n",
        "# input_length is size of review text after tokenization and pad sequance\n",
        "embedding_dim = 100\n",
        "\n",
        "\n",
        "model.add(layers.Embedding(input_dim=vocab_size,\n",
        "                           output_dim=embedding_dim,\n",
        "                           input_length=maxlen))\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(10, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='softmax'))\n",
        "\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 1000, 100)         2391000   \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 100000)            0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                1000010   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 3,391,021\n",
            "Trainable params: 3,391,021\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpANbz7SpvCj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PG11pjVTr7QY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        },
        "outputId": "aa7860f2-ba2c-4739-ed61-8bb9a19aa4b6"
      },
      "source": [
        "history = model.fit(X_train1, y_train,\n",
        "                    epochs=20,verbose=True,\n",
        "                    validation_data=(X_test1, y_test),\n",
        "                    batch_size=10)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-97099be39ade>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                     batch_size=10)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    783\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    784\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 785\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    786\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    625\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    504\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    505\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 506\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2444\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2445\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2446\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2447\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2776\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2777\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2778\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2779\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2665\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2666\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2667\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2668\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2669\u001b[0m         \u001b[0;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:505 train_function  *\n        outputs = self.distribute_strategy.run(\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:467 train_step  **\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/compile_utils.py:204 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:143 __call__\n        losses = self.call(y_true, y_pred)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:246 call\n        return self.fn(y_true, y_pred, **self._fn_kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:1527 categorical_crossentropy\n        return K.categorical_crossentropy(y_true, y_pred, from_logits=from_logits)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py:4561 categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_shape.py:1117 assert_is_compatible_with\n        raise ValueError(\"Shapes %s and %s are incompatible\" % (self, other))\n\n    ValueError: Shapes (None, 5) and (None, 1) are incompatible\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4avc-q_oo1nj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCDh6tAeoAcG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIDyHyO5nk36",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvPx3L5Nnex1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gzH4iNWklQY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_2zGcE_khVn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xc4bq81jU2J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wuvl8Ebqi9rc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GttL0CB6iu9a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sS6wkbGGisQe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XLYqKT0ijqq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Nj_R10kihLv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDFWlvBtid4s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYYkLh4SZvao",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7KYwvjCZSjM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}