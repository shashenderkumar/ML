{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"file_dir = '/kaggle/input/learn-ai-bbc/'\ntrain_file = file_dir + 'BBC News Train.csv'\ntest_file = file_dir + 'BBC News Test.csv'\nsubmit_file = file_dir + 'BBC News Sample Solution.csv'\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train = pd.read_csv(train_file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train.Category.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#max length of Text present in data\nsrt =[]\nfor s in data_train['Text']:\n    srt.append(len(s.split(' ')))\nsrt.sort()\nsrt[-1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"****Split data in train and test********"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data_train['Text'].values\n# X = data_train.iloc[0:2].values\ny = data_train['Category'].values\n\ny_t = np.array(y)\n# integer encode\nle = LabelEncoder()\ninteger_encoded = le.fit_transform(y_t)\n# binary encode\nohe = OneHotEncoder(sparse=False)\ninteger_encoded = integer_encoded.reshape(len(integer_encoded), 1)\ny_t1 = ohe.fit_transform(integer_encoded)\nprint(y_t1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y_t1, test_size = 0.20, random_state = 1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train.size)\nprint(X_test.size)\nprint(len(data_train))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Tokenize Data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer(num_words=5000)\n\ntokenizer.fit_on_texts(X_train)\n\nX_train1 = tokenizer.texts_to_sequences(X_train)\nX_test1 = tokenizer.texts_to_sequences(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size = len(tokenizer.word_index) + 1\nprint(vocab_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train[1:6])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train1[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**PADS Sequence**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# have to check the value of this hyperparameter - calculated above 3519\nmaxlen = 3100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train1 = pad_sequences(X_train1, padding= 'post', maxlen = maxlen)\nX_test1 = pad_sequences(X_test1, padding= 'post', maxlen = maxlen)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train1[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Model Creation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nm = tf.keras.models\nlayers = tf.keras.layers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = m.Sequential()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# vocab size is 22811\n# input_length is size of review text after tokenization and pad sequance\nembedding_dim = 75\n\n\nmodel.add(layers.Embedding(input_dim=vocab_size,\n                           output_dim=embedding_dim,\n                           input_length=maxlen))\n\n#model.add(layers.Flatten())\nmodel.add(layers.GlobalMaxPool1D())\n#model.add(layers.GlobalAveragePooling1D())\nmodel.add(layers.Dense(1024, activation='relu'))\n#model.add(layers.Dense(512, activation='relu'))\nmodel.add(layers.Dense(100, activation='relu'))\nmodel.add(layers.Dense(5, activation='softmax'))\n\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"adam = tf.keras.optimizers.Adam(lr=0.0005)\nmodel.compile(optimizer= adam,\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_train1, y_train,\n                    epochs=18,verbose=True,\n                    validation_data=(X_test1, y_test),\n                    batch_size=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_embedding_matrix(filepath, word_index, embedding_dim):\n    \n    vocab_size = len(word_index) + 1 \n    # Adding again 1 because of reserved 0 index\n    \n    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n\n    with open(filepath) as file:\n        \n        for line in file:\n            \n            # Fetching word and weights of that word from pre-trained file\n            word, *vector = line.split()\n            \n            # putting word only if it exist in text for which model is created\n            if word in word_index:\n                # Find token number of each word from Toekenzier\n                idx = word_index[word]\n                #print(\"{} {} \".format(word,idx))\n                # Create matrix with toekn and its vector from pre-trained weights\n                embedding_matrix[idx] = np.array(vector, dtype=np.float32)[:embedding_dim]\n    return embedding_matrix\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix = create_embedding_matrix(filePath,\n                                          tokenizer.word_index,\n                                          embedding_dim)\n\n\nmodel.add(layers.Embedding(input_dim=vocab_size,\n                           output_dim=embedding_dim,\n                           input_length=maxlen))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}